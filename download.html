<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="">
        <meta name="author" content="">
        <!-- <link rel="shortcut icon" href="assets/ico/favicon.png"> -->
        <title>HABits Lab | The Northwestern University Health Aware Bits Lab</title>
        <!-- Bootstrap Core CSS -->
        <link href="assets/bootstrap/css/bootstrap.css" rel="stylesheet">
        <!-- Plugins CSS -->
        <link href="assets/UItoTop/css/ui.totop.css" rel="stylesheet">
        <link href="assets/prettyPhoto/css/prettyPhoto.css" rel="stylesheet">
        <!-- REVOLUTION BANNER CSS SETTINGS -->
        <link rel="stylesheet" type="text/css" href="assets/rs-plugin/css/settings.css" media="screen" />
        <!-- Font Awesome  -->
        <link href="assets/font-awesome/css/font-awesome.min.css" rel="stylesheet">
        <!-- Custom Stylesheet For This Template -->
        <link href="assets/css/stylesheet.css" rel="stylesheet">
        <link href="assets/css/skins.css" rel="stylesheet">
        <link rel="stylesheet" href="assets/css/styles.css" type="text/css" media="screen"/>
        <!-- Google Fonts -->
        <link href='https://fonts.googleapis.com/css?family=Raleway:600,700,100,900,200,500,800,400,300|Montserrat:400,700' rel='stylesheet' type='text/css'>
        <script src="assets/js/angular.min.js"></script>
        <script src="js/publicationCtrl.js"></script>
        <script type="text/javascript" src="js/nav.js"></script>
        <script type="text/javascript" src="js/footer.js"></script>
    </head>
    <body ng-app="publicationApp" ng-controller="publicationCtrl" onload="setNav();setFooter()">
        <div id="utter-wrapper" class="color-skin-3">
          <header id="header" class="header">
            <div id="mobile-links">
                <a href="#mobile-nav" class="mobile-link mobile-nav-link"><span class="hide-label">Menu</span></a>
                <nav id="mobile-nav">
                  <ul>
                      <li><a href="index.html">Home</a></li>
                      <li><a href="team.html">People</a></li>
                      <li><a href="research.html">Research</a></li>
                      <li><a href="publications.html">Publication</a></li>
                      <li><a href="download.html">Download</a></li>
                      <li><a href="portfolio.html">Projects</a></li>
                      <li><a href="contact.html">Contact/Join</a></li>
                      <li><a href="news.html">Latest News</a></li>
                    </ul>
                    <div id="mobile-nav-bottom">
                        <!-- links from #top-bar #right -->
                        <ul id="mobile-nav-bottom-left">
                          <li><a href="http://www.nalshurafa.com/">Lab Director's Page</a></li>
                        </ul>

                    </div>
                </nav>
            </div>
          </header>
            <!-- /#header -->
                <!-- /.page-title-wrapper -->
                    <div class="container pad-25">
                      <div class="subpage-title">
                          <h5>Public Dataset of Accelerometer and Gyroscope Data for Human Feeding Gesture Detection</h5>
                      </div>

                      <h5>Download: <u><a href="https://drive.google.com/file/d/0B31ueLWlMaKlZXhQMTJvMmZvS0U/view?usp=sharing">ZIP</a></u>&nbsp;&nbsp;<u><a href="https://drive.google.com/file/d/0B31ueLWlMaKlcjdlT25Rd2hEdE0/view?usp=sharing">TAR</a></u></h5>
                      <br><br>

<u>1. What is it?</u>
<br><br>
The Public Dataset of Accelerometer and Gyroscope Data for Human Feeding Gesture Detection is a public collection of labelled accelerometer and gyroscope data recordings to be used for the creation and validation of acceleration and gyroscope models of human feeding primitives.
<br><br>
Three protocols/experiments are followed for collection of the data sets: inlab highly structured, inlab structured and inlab unstructured.  
<br><br>
Participants (10 in total, 1 female and 9 males, mean age=24.2 years) were requested to wear a wrist-worn sensor (Microsoft Band 2) on their dominant hand. A Logitech C615 HD webcam camera was placed on the side of the non-dominant hand in order to see the plate, both hands and the participants face. Three trained researchers are used to correctly label the video collected into feeding and non-feeding gestures. Chronoviz was used to label data. 
<br><br>
Although the experiments primarily focus on detecting feeding gestures, the variety of the labels include start and end times for: eating, drinking, feeding gesture (hand to mouth and back to food), and non-feeding gesture (hand up, and back down but not feeding related). All the participants were right-handed, although a few did consume food with their left hand during unstructured eating, and as in this study we do not capture the participants' use of their non-dominant hands, feeding gestures performed with the left-hand represent uncaptured data. The entire session was recorded using a camera. The annotated recordings serve as ground truth for our models. The recorded data is annotated with time-synchronized ground truth labels.
<br><br>
In the highly structured test, participants are asked to perform 10 gestures of each of the activities using only a utensil and imaginary food (no food is presented in this experiment). The participants are asked to follow a script instructing them when to perform each gesture, separating each gesture by 10 seconds of motionless activity. This makes it possible for a simple script to filter out the motion into perfect feeding gesture segments. This is essential to see if food is necessary to train a model, or whether pretend eating is sufficient.
<br><br>
In the in-lab structured test, participants performed all the activities (eating and non-eating)(randomized for each participant) for exactly two minutes each. The participants were instructed to follow a script on a developed interface, which reminded them of the current activity, and instructed them to consume the foods as freely as possible. This test reflected more natural feeding gestures from each participant. Caloric intake was calculated by weighing food before and after the experiment and the number of feeding gestures was obtained from the video. The number of calories and the number of gestures were recorded for each participant. At the end of the experiment participants were asked about the foods they like to eat and what T.V. show or movie they would like to watch. We used that information to tailor the unstructured experiment for each participant.
<br><br>
Participants were asked to return to the lab on another date in the evening to eat food in a lab setting. After putting on the sensors, they were asked if they were full; if they were not full, they were provided with a meal of their choice to eat their fill, then they were taken to a room with a big screen T.V. to watch their favorite T.V. show/movie. If they were full, then they were led directly to the T.V. room. While watching T.V., participants were presented with their favorite snacks (identified in in-lab structured test). During both the pre-load and snacking phases participants provided a large variety of gestures such as unwrapping food and finger licking that challenged the robustness of our algorithms.
<br><br>

<u>2. Version</u>
<br><br>
Version: 1
Released on: 06/21/2017
<br><br>
<u>3. Documentation</u>
<br><br>
Up-to-date documentation for this release is provided in the file MANUAL.txt
<br><br>
<u>4. Installation and usage</u>
<br><br>
This dataset does not require any installation and can be viewed with spreadsheet application that can open .csv files.
<br><br>
<u>5. Licensing</u>
<br><br>
This dataset is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY, including the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
The authors allow the users of the Public Dataset of Accelerometer and Gyroscope Data for Human feeding gestures detection to use and modify it for their own research. Any commercial application, redistribution, etc... has to be arranged between users and authors individually.
<br><br>
For further license information, please contact the authors.
<br><br>
<u>6. Authors contacts</u>
<br><br>
If you want to be informed about db updates and new code releases, obtain further information about the provided db, or contribute to its development please write to:
<br><br>
<div style="margin-left: 10px">
HABits Lab
<br><br>
680 N. Lakeshore Dr., Suite 1400
<br><br>
Chicago, IL 60611
<br><br>
Phone: (312) 503-4517
<br><br>
nabil@northwestern.edu
</div>
<br><br>
                        

                      </div><!-- /#container -->
                      <div id="appendFooter" style="padding-top:25px"></div>
                    </div><!-- /#utter wrapper -->
        </div>
        <!-- /#utter-wrapper -->
        <!-- Bootstrap JS & Others JavaScript Plugins
            ================================================== -->
        <!-- Placed At The End Of The Document So Page Loads Faster -->
        <script src="assets/js/jquery-2.0.3.min.js"></script>
        <script src="assets/js/jquery-migrate-1.2.1.min.js"></script>
        <script src="assets/bootstrap/js/bootstrap.min.js"></script>

        <!-- jQuery REVOLUTION Slider  -->
        <script src="assets/rs-plugin/js/jquery.themepunch.tools.min.js"></script>
        <script src="assets/rs-plugin/js/jquery.themepunch.revolution.min.js"></script>

        <script src="assets/carouFredSel-6.2.1/jquery.carouFredSel-6.2.1.js"></script>
        <script src="assets/prettyPhoto/js/jquery.prettyPhoto.js"></script>
        <script src="assets/jflickrfeed/jflickrfeed.min.js"></script>
        <script src="assets/UItoTop/js/easing.js"></script>
        <script src="assets/UItoTop/js/jquery.ui.totop.min.js"></script>
        <script src="assets/isotope-site/jquery.isotope.min.js"></script>
        <script src="assets/FitVids.js/jquery.fitvids.js"></script>
        <script type="text/javascript">
            $(document).ready(function () {
                $(".responsive-video-wrapper").fitVids();
            });
        </script>
        <script type="text/javascript">
            $(document).ready(function () {
                // PrettyPhoto
                $("a[rel^='prettyPhoto']").prettyPhoto({
                    theme: 'light_square',
                    social_tools: false
                });
            });
            // Isotope Portfolio
            var $container = $('#isotope-publication');
            var $filter = $('.portfolio-filter');
            $(window).load(function () {
                // Initialize Isotope
                $container.isotope({
                    itemSelector: '.publication-selector'
                });
                $('.portfolio-filter a').click(function () {
                    var selector = $(this).attr('data-filter');
                    $container.isotope({ filter: selector });
                    return false;
                });
                $filter.find('a').click(function () {
                    var selector = $(this).attr('data-filter');
                    $filter.find('a').parent().removeClass('active');
                    $(this).parent().addClass('active');
                });
            });
            $(window).smartresize(function () {
                $container.isotope('reLayout');
            });
        </script>
        <script src="js/scripts.js"></script>
        <!-- Custom Script For This Template -->
        <script src="assets/js/script.js"></script>
    </body>
</html>
